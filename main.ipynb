{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import scipy.fft\n",
    "import soundfile as sf\n",
    "import sounddevice as sd\n",
    "import librosa.display\n",
    "from spleeter.separator import Separator\n",
    "\n",
    "# Using embedded configuration.\n",
    "separator = Separator('spleeter:2stems-16kHz')\n",
    "waveform, fs = sf.read('./data/song1.mp3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the separation :\n",
    "prediction = separator.separate(waveform)\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)\n",
    "sf.write('vocals/vocal1.wav',prediction['vocals'],fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "model = load_silero_vad()\n",
    "wav = read_audio('./vocals/vocal1.wav')\n",
    "speech_timestamps = get_speech_timestamps(\n",
    "  wav,\n",
    "  model,\n",
    "  return_seconds=True,  # Return speech timestamps in seconds (default is samples)\n",
    ")\n",
    "speech_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crepe\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "\n",
    "sr, audio = wavfile.read('./vocals/vocal1.wav')\n",
    "with tf.Graph().as_default():\n",
    "    time, frequency, confidence, activation = crepe.predict(audio, sr, viterbi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time, frequency, confidence, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "Y, fs = librosa.load('./vocals/vocal1.wav')\n",
    "\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=Y, sr=fs)\n",
    "\n",
    "# Pitch contour plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spectrogram, ref=np.max), sr=fs, hop_length=512, y_axis=\"mel\", x_axis=\"time\")\n",
    "plt.plot(time, frequency, label='Pitch (Hz)', color='b')\n",
    "#plt.xlabel('Time (s)')\n",
    "#plt.ylabel('Frequency (Hz)')\n",
    "plt.title('Pitch Contour')\n",
    "#plt.legend()\n",
    "#plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "Y, fs = librosa.load('./vocals/vocal1.wav')\n",
    "\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=Y, sr=fs)\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spectrogram, ref=np.max), sr=fs, hop_length=512, y_axis=\"mel\", x_axis=\"time\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel-Spectrogram\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
